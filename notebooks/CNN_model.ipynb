{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#start\n"
      ],
      "metadata": {
        "id": "6F8NDZEtr3Ri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "VgAGkvw4boie",
        "outputId": "35c77e66-6fbc-4314-b9dd-ae96abd056a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "metadata": {
        "id": "dIgGZ0wUbsYs"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "A2zNsjKErvRc",
        "outputId": "ec0082dc-af35-4f94-bd6a-e061d1955ea5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 709 images belonging to 5 classes.\n",
            "Found 210 images belonging to 5 classes.\n",
            "Found 2805 images belonging to 5 classes.\n",
            "Found 725 images belonging to 5 classes.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Paths\n",
        "output_dir = '/content/drive/MyDrive/Tomato_dataset/cnn_crops'\n",
        "\n",
        "# Data augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=15,\n",
        "    horizontal_flip=True,\n",
        "    brightness_range=[0.8,1.2],\n",
        "    zoom_range=0.2\n",
        ")\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Load datasets\n",
        "datasets_dict = {\n",
        "    'ripe': {\n",
        "        'train': train_datagen.flow_from_directory(\n",
        "            os.path.join(output_dir, 'ripe/train'),\n",
        "            target_size=(224,224),\n",
        "            batch_size=16,\n",
        "            class_mode='categorical'\n",
        "        ),\n",
        "        'val': val_datagen.flow_from_directory(\n",
        "            os.path.join(output_dir, 'ripe/val'),\n",
        "            target_size=(224,224),\n",
        "            batch_size=16,\n",
        "            class_mode='categorical'\n",
        "        )\n",
        "    },\n",
        "    'green': {\n",
        "        'train': train_datagen.flow_from_directory(\n",
        "            os.path.join(output_dir, 'green/train'),\n",
        "            target_size=(224,224),\n",
        "            batch_size=16,\n",
        "            class_mode='categorical'\n",
        "        ),\n",
        "        'val': val_datagen.flow_from_directory(\n",
        "            os.path.join(output_dir, 'green/val'),\n",
        "            target_size=(224,224),\n",
        "            batch_size=16,\n",
        "            class_mode='categorical'\n",
        "        )\n",
        "    }\n",
        "}\n",
        "\n",
        "# Define CNN model\n",
        "def create_cnn(num_classes):\n",
        "    model = Sequential([\n",
        "        Conv2D(16, (3,3), activation='relu', padding='same', input_shape=(224,224,3)),\n",
        "        MaxPooling2D(2,2),\n",
        "        Conv2D(32, (3,3), activation='relu', padding='same'),\n",
        "        MaxPooling2D(2,2),\n",
        "        Flatten(),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WZmALZhIjcls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Train Ripe Tomato CNN\n",
        "\n",
        "ripe_train = datasets_dict['ripe']['train']\n",
        "ripe_val = datasets_dict['ripe']['val']\n",
        "num_classes = len(ripe_train.class_indices)\n",
        "\n",
        "ripe_model = create_cnn(num_classes)\n",
        "\n",
        "ripe_save_path = '/content/drive/MyDrive/Tomato_dataset/models/ripe_cnn.h5'\n",
        "\n",
        "history_ripe = ripe_model.fit(\n",
        "    ripe_train,\n",
        "    validation_data=ripe_val,\n",
        "    epochs=20,\n",
        "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)]\n",
        ")\n",
        "\n",
        "ripe_model.save(ripe_save_path)\n",
        "print(f\"Ripe CNN trained and saved to {ripe_save_path}\")\n"
      ],
      "metadata": {
        "id": "6zStzTMgwChy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9e5aaa3-a313-497c-cba6-3a4b0e22e285"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 4s/step - accuracy: 0.4483 - loss: 2.7629 - val_accuracy: 0.5381 - val_loss: 1.1844\n",
            "Epoch 2/20\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 1s/step - accuracy: 0.5841 - loss: 1.1701 - val_accuracy: 0.5381 - val_loss: 1.1213\n",
            "Epoch 3/20\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 1s/step - accuracy: 0.5414 - loss: 1.1646 - val_accuracy: 0.5429 - val_loss: 1.1095\n",
            "Epoch 4/20\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 1s/step - accuracy: 0.5113 - loss: 1.2125 - val_accuracy: 0.5095 - val_loss: 1.1360\n",
            "Epoch 5/20\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 1s/step - accuracy: 0.5586 - loss: 1.1293 - val_accuracy: 0.5190 - val_loss: 1.1244\n",
            "Epoch 6/20\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 1s/step - accuracy: 0.6097 - loss: 1.0284 - val_accuracy: 0.5333 - val_loss: 1.1447\n",
            "Epoch 7/20\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 1s/step - accuracy: 0.5598 - loss: 1.0378 - val_accuracy: 0.5381 - val_loss: 1.1047\n",
            "Epoch 8/20\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 1s/step - accuracy: 0.5916 - loss: 1.0333 - val_accuracy: 0.5429 - val_loss: 1.1116\n",
            "Epoch 9/20\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 1s/step - accuracy: 0.5847 - loss: 1.0278 - val_accuracy: 0.5429 - val_loss: 1.0946\n",
            "Epoch 10/20\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 1s/step - accuracy: 0.5691 - loss: 0.9984 - val_accuracy: 0.5238 - val_loss: 1.0930\n",
            "Epoch 11/20\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 1s/step - accuracy: 0.5837 - loss: 0.9774 - val_accuracy: 0.5333 - val_loss: 1.1004\n",
            "Epoch 12/20\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 1s/step - accuracy: 0.5402 - loss: 1.0476 - val_accuracy: 0.5333 - val_loss: 1.1358\n",
            "Epoch 13/20\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 1s/step - accuracy: 0.5831 - loss: 1.0137 - val_accuracy: 0.5381 - val_loss: 1.1432\n",
            "Epoch 14/20\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 1s/step - accuracy: 0.5873 - loss: 0.9772 - val_accuracy: 0.5571 - val_loss: 1.1339\n",
            "Epoch 15/20\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 1s/step - accuracy: 0.6050 - loss: 0.9631 - val_accuracy: 0.5381 - val_loss: 1.1601\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ripe CNN trained and saved to /content/drive/MyDrive/Tomato_dataset/models/ripe_cnn.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ripe_model.save('/content/drive/MyDrive/Tomato_dataset/models/ripe_cnn.keras')\n"
      ],
      "metadata": {
        "id": "DOQzagJOjyCm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8ek89wc0jMO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Train Green Tomato CNN\n",
        "\n",
        "green_train = datasets_dict['green']['train']\n",
        "green_val = datasets_dict['green']['val']\n",
        "num_classes = len(green_train.class_indices)\n",
        "\n",
        "green_model = create_cnn(num_classes)\n",
        "\n",
        "green_save_path = '/content/drive/MyDrive/Tomato_dataset/models/green_cnn.h5'\n",
        "\n",
        "history_green = green_model.fit(\n",
        "    green_train,\n",
        "    validation_data=green_val,\n",
        "    epochs=20,\n",
        "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)]\n",
        ")\n",
        "\n",
        "green_model.save(green_save_path)\n",
        "print(f\"Green CNN trained and saved to {green_save_path}\")\n"
      ],
      "metadata": {
        "id": "j2HqJdqydRsw",
        "outputId": "ffe9d9cc-40d4-4567-a62b-c3108c3f911c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m 50/176\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10:55\u001b[0m 5s/step - accuracy: 0.4687 - loss: 3.7540"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "green_model.save('/content/drive/MyDrive/Tomato_dataset/models/green_cnn.keras')\n"
      ],
      "metadata": {
        "id": "x10a6l5xj4au"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GXf5AVc5j4kh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}