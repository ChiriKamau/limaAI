{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChiriKamau/limaAI/blob/main/notebooks/CNN_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#start\n"
      ],
      "metadata": {
        "id": "6F8NDZEtr3Ri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tv02Tq70v6dx",
        "outputId": "9a632ec6-5414-47d1-8528-95d99dfa7e58"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Collecting astunparse>=1.6.0 (from tensorflow)\n",
            "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
            "  Downloading flatbuffers-25.9.23-py2.py3-none-any.whl.metadata (875 bytes)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Collecting google_pasta>=0.1.1 (from tensorflow)\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
            "Collecting libclang>=13.0.0 (from tensorflow)\n",
            "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: opt_einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf>=5.28.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (6.32.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing_extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.75.1)\n",
            "Collecting tensorboard~=2.20.0 (from tensorflow)\n",
            "  Downloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: keras>=3.10.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.0)\n",
            "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n",
            "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow)\n",
            "  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.10.0->tensorflow) (14.2.0)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.10.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.10.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/lib/python3/dist-packages (from tensorboard~=2.20.0->tensorflow) (3.3.6)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.20.0->tensorflow) (11.3.0)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.20.0->tensorflow)\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting werkzeug>=1.0.1 (from tensorboard~=2.20.0->tensorflow)\n",
            "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.10.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.10.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.2)\n",
            "Downloading tensorflow-2.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (620.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m620.7/620.7 MB\u001b[0m \u001b[31m787.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Downloading flatbuffers-25.9.23-py2.py3-none-any.whl (30 kB)\n",
            "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m88.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m129.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m122.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: libclang, flatbuffers, wheel, werkzeug, tensorboard-data-server, google_pasta, tensorboard, astunparse, tensorflow\n",
            "Successfully installed astunparse-1.6.3 flatbuffers-25.9.23 google_pasta-0.2.0 libclang-18.1.1 tensorboard-2.20.0 tensorboard-data-server-0.7.2 tensorflow-2.20.0 werkzeug-3.1.3 wheel-0.45.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "VgAGkvw4boie",
        "outputId": "4250ab23-ecca-484f-b7f0-8929cc503279",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n"
      ],
      "metadata": {
        "id": "dIgGZ0wUbsYs",
        "outputId": "4df6f97f-ae1b-422f-ee4f-fe1a21222475",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jax/_src/cloud_tpu_init.py:82: UserWarning: Transparent hugepages are not enabled. TPU runtime startup and shutdown time should be significantly improved on TPU v5e and newer. If not already set, you may need to enable transparent hugepages in your VM image (sudo sh -c \"echo always > /sys/kernel/mm/transparent_hugepage/enabled\")\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "A2zNsjKErvRc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad3a86c8-1faa-4724-c659-b4c0b97a1efd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1579 images belonging to 3 classes.\n",
            "Found 384 images belonging to 3 classes.\n",
            "Found 4344 images belonging to 4 classes.\n",
            "Found 946 images belonging to 4 classes.\n",
            "ripe - train: 1579 images across 3 classes\n",
            "ripe - val: 384 images across 3 classes\n",
            "green - train: 4344 images across 4 classes\n",
            "green - val: 946 images across 4 classes\n"
          ]
        }
      ],
      "source": [
        "\n",
        "output_dir = '/content/drive/MyDrive/Tomato_dataset/cnn_crops'\n",
        "\n",
        "# Data augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=15,\n",
        "    horizontal_flip=True,\n",
        "    brightness_range=[0.8,1.2],\n",
        "    zoom_range=0.2\n",
        ")\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Load datasets\n",
        "datasets_dict = {\n",
        "    'ripe': {\n",
        "        'train': train_datagen.flow_from_directory(\n",
        "            os.path.join(output_dir, 'ripe/train'),\n",
        "            target_size=(224,224),\n",
        "            batch_size=16,\n",
        "            class_mode='categorical'\n",
        "        ),\n",
        "        'val': val_datagen.flow_from_directory(\n",
        "            os.path.join(output_dir, 'ripe/val'),\n",
        "            target_size=(224,224),\n",
        "            batch_size=16,\n",
        "            class_mode='categorical'\n",
        "        )\n",
        "    },\n",
        "    'green': {\n",
        "        'train': train_datagen.flow_from_directory(\n",
        "            os.path.join(output_dir, 'green/train'),\n",
        "            target_size=(224,224),\n",
        "            batch_size=16,\n",
        "            class_mode='categorical'\n",
        "        ),\n",
        "        'val': val_datagen.flow_from_directory(\n",
        "            os.path.join(output_dir, 'green/val'),\n",
        "            target_size=(224,224),\n",
        "            batch_size=16,\n",
        "            class_mode='categorical'\n",
        "        )\n",
        "    }\n",
        "}\n",
        "\n",
        "# Check number of images per class\n",
        "for color in datasets_dict:\n",
        "    for split in datasets_dict[color]:\n",
        "        dataset = datasets_dict[color][split]\n",
        "        print(f\"{color} - {split}: {dataset.samples} images across {dataset.num_classes} classes\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Check number of images per class\n",
        "# -----------------------------\n",
        "for color in datasets_dict:\n",
        "    for split in datasets_dict[color]:\n",
        "        dataset = datasets_dict[color][split]\n",
        "        print(f\"\\n{color} - {split}:\")\n",
        "        for class_name, count in zip(dataset.class_indices.keys(), dataset.classes):\n",
        "            # Count images per class\n",
        "            class_count = sum(dataset.classes == dataset.class_indices[class_name])\n",
        "            print(f\"  {class_name}: {class_count} images\")\n"
      ],
      "metadata": {
        "id": "r9jrSBeSwlIe",
        "outputId": "92e4a564-a742-40b1-cc1c-3f356a97a653",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ripe - train:\n",
            "  R_ber: 447 images\n",
            "  R_healthy: 768 images\n",
            "  R_spots: 364 images\n",
            "\n",
            "ripe - val:\n",
            "  R_ber: 80 images\n",
            "  R_healthy: 194 images\n",
            "  R_spots: 110 images\n",
            "\n",
            "green - train:\n",
            "  G_ber: 475 images\n",
            "  G_healthy: 2141 images\n",
            "  G_lateblight: 191 images\n",
            "  G_spots: 1537 images\n",
            "\n",
            "green - val:\n",
            "  G_ber: 41 images\n",
            "  G_healthy: 481 images\n",
            "  G_lateblight: 41 images\n",
            "  G_spots: 383 images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import class_weight\n",
        "import numpy as np\n",
        "\n",
        "# Function to compute class weights for a generator\n",
        "def get_class_weights(generator):\n",
        "    y_train = generator.classes\n",
        "    classes = np.unique(y_train)\n",
        "    weights = class_weight.compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n",
        "    return dict(zip(classes, weights))\n",
        "\n",
        "# Compute class weights\n",
        "ripe_class_weights = get_class_weights(datasets_dict['ripe']['train'])\n",
        "green_class_weights = get_class_weights(datasets_dict['green']['train'])\n",
        "\n",
        "print(\"Ripe class weights:\", ripe_class_weights)\n",
        "print(\"Green class weights:\", green_class_weights)"
      ],
      "metadata": {
        "id": "WZmALZhIjcls",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b0e664e-a500-4bcc-838a-432c02c50348"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ripe class weights: {np.int32(0): np.float64(1.1774794929157346), np.int32(1): np.float64(0.6853298611111112), np.int32(2): np.float64(1.445970695970696)}\n",
            "Green class weights: {np.int32(0): np.float64(2.2863157894736843), np.int32(1): np.float64(0.507239607659972), np.int32(2): np.float64(5.68586387434555), np.int32(3): np.float64(0.7065712426805465)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6036f5a"
      },
      "source": [
        "def create_cnn(num_classes):\n",
        "    \"\"\"\n",
        "    Creates a simple CNN model for image classification.\n",
        "\n",
        "    Args:\n",
        "        num_classes: The number of output classes.\n",
        "\n",
        "    Returns:\n",
        "        A Keras Sequential model.\n",
        "    \"\"\"\n",
        "    model = Sequential([\n",
        "        Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Conv2D(64, (3, 3), activation='relu'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Conv2D(128, (3, 3), activation='relu'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Flatten(),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Light augmentation for memory efficiency\n",
        "ripe_train_gen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=15,\n",
        "    horizontal_flip=True,\n",
        "    brightness_range=[0.8,1.2],\n",
        "    zoom_range=0.2\n",
        ").flow_from_directory(\n",
        "    '/content/drive/MyDrive/Tomato_dataset/cnn_crops/ripe/train',\n",
        "    target_size=(224,224),\n",
        "    batch_size=16,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "ripe_val_gen = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
        "    '/content/drive/MyDrive/Tomato_dataset/cnn_crops/ripe/val',\n",
        "    target_size=(224,224),\n",
        "    batch_size=16,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Create model\n",
        "num_classes = len(ripe_train_gen.class_indices)\n",
        "ripe_model = create_cnn(num_classes)\n",
        "\n",
        "# Train with class weights\n",
        "ripe_save_path = '/content/drive/MyDrive/Tomato_dataset/models/ripe_cnn(v2).h5'\n",
        "history_ripe = ripe_model.fit(\n",
        "    ripe_train_gen,\n",
        "    validation_data=ripe_val_gen,\n",
        "    epochs=20,\n",
        "    class_weight=ripe_class_weights,\n",
        "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)]\n",
        ")\n",
        "\n",
        "ripe_model.save(ripe_save_path)\n",
        "print(f\"Ripe CNN trained and saved to {ripe_save_path}\")\n",
        "\n",
        "ripe_model.save('/content/drive/MyDrive/Tomato_dataset/models/ripe_cnn(v2).keras')\n",
        "\n"
      ],
      "metadata": {
        "id": "6zStzTMgwChy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95442710-9222-44ae-b0c1-1c753421b56c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1579 images belonging to 3 classes.\n",
            "Found 384 images belonging to 3 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m543s\u001b[0m 5s/step - accuracy: 0.4443 - loss: 1.2823 - val_accuracy: 0.2760 - val_loss: 1.1938\n",
            "Epoch 2/20\n",
            "\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 721ms/step - accuracy: 0.4703 - loss: 1.0050 - val_accuracy: 0.4948 - val_loss: 0.9824\n",
            "Epoch 3/20\n",
            "\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 729ms/step - accuracy: 0.5132 - loss: 0.9969 - val_accuracy: 0.5625 - val_loss: 0.9645\n",
            "Epoch 4/20\n",
            "\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 726ms/step - accuracy: 0.5585 - loss: 0.9344 - val_accuracy: 0.4479 - val_loss: 1.0137\n",
            "Epoch 5/20\n",
            "\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 723ms/step - accuracy: 0.5428 - loss: 0.9053 - val_accuracy: 0.5677 - val_loss: 0.9366\n",
            "Epoch 6/20\n",
            "\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 726ms/step - accuracy: 0.5392 - loss: 0.8716 - val_accuracy: 0.5521 - val_loss: 0.9678\n",
            "Epoch 7/20\n",
            "\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 724ms/step - accuracy: 0.5785 - loss: 0.8647 - val_accuracy: 0.5599 - val_loss: 0.9393\n",
            "Epoch 8/20\n",
            "\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 723ms/step - accuracy: 0.5752 - loss: 0.8572 - val_accuracy: 0.5312 - val_loss: 0.9565\n",
            "Epoch 9/20\n",
            "\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 726ms/step - accuracy: 0.5704 - loss: 0.8495 - val_accuracy: 0.5521 - val_loss: 0.9435\n",
            "Epoch 10/20\n",
            "\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 734ms/step - accuracy: 0.5873 - loss: 0.8311 - val_accuracy: 0.5417 - val_loss: 0.9359\n",
            "Epoch 11/20\n",
            "\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 722ms/step - accuracy: 0.5928 - loss: 0.8175 - val_accuracy: 0.4922 - val_loss: 0.9781\n",
            "Epoch 12/20\n",
            "\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 722ms/step - accuracy: 0.6174 - loss: 0.8018 - val_accuracy: 0.5260 - val_loss: 0.9428\n",
            "Epoch 13/20\n",
            "\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 721ms/step - accuracy: 0.5987 - loss: 0.7864 - val_accuracy: 0.5443 - val_loss: 0.9051\n",
            "Epoch 14/20\n",
            "\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 717ms/step - accuracy: 0.6302 - loss: 0.7639 - val_accuracy: 0.4583 - val_loss: 1.0212\n",
            "Epoch 15/20\n",
            "\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 723ms/step - accuracy: 0.5727 - loss: 0.8015 - val_accuracy: 0.5495 - val_loss: 0.9646\n",
            "Epoch 16/20\n",
            "\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 716ms/step - accuracy: 0.6136 - loss: 0.7854 - val_accuracy: 0.5677 - val_loss: 0.9102\n",
            "Epoch 17/20\n",
            "\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 724ms/step - accuracy: 0.5972 - loss: 0.7714 - val_accuracy: 0.5755 - val_loss: 0.8844\n",
            "Epoch 18/20\n",
            "\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 717ms/step - accuracy: 0.6456 - loss: 0.7349 - val_accuracy: 0.5625 - val_loss: 0.9130\n",
            "Epoch 19/20\n",
            "\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 714ms/step - accuracy: 0.6493 - loss: 0.7192 - val_accuracy: 0.5625 - val_loss: 0.8806\n",
            "Epoch 20/20\n",
            "\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 721ms/step - accuracy: 0.6779 - loss: 0.6929 - val_accuracy: 0.5781 - val_loss: 0.9074\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ripe CNN trained and saved to /content/drive/MyDrive/Tomato_dataset/models/ripe_cnn(v2).h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8ek89wc0jMO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Light augmentation for memory efficiency\n",
        "green_train_gen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=15,\n",
        "    horizontal_flip=True,\n",
        "    brightness_range=[0.8,1.2],\n",
        "    zoom_range=0.2\n",
        ").flow_from_directory(\n",
        "    '/content/drive/MyDrive/Tomato_dataset/cnn_crops/green/train',\n",
        "    target_size=(224,224),\n",
        "    batch_size=16,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "green_val_gen = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
        "    '/content/drive/MyDrive/Tomato_dataset/cnn_crops/green/val',\n",
        "    target_size=(224,224),\n",
        "    batch_size=16,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Create model\n",
        "num_classes = len(green_train_gen.class_indices)\n",
        "green_model = create_cnn(num_classes)\n",
        "\n",
        "# Train with class weights\n",
        "green_save_path = '/content/drive/MyDrive/Tomato_dataset/models/green_cnn(v2).h5'\n",
        "history_green = green_model.fit(\n",
        "    green_train_gen,\n",
        "    validation_data=green_val_gen,\n",
        "    epochs=20,\n",
        "    class_weight=green_class_weights,\n",
        "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)]\n",
        ")\n",
        "\n",
        "green_model.save(green_save_path)\n",
        "print(f\"Green CNN trained and saved to {green_save_path}\")\n",
        "\n",
        "green_model.save('/content/drive/MyDrive/Tomato_dataset/models/green_cnn(v2).keras')"
      ],
      "metadata": {
        "id": "j2HqJdqydRsw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cce514de-47a0-41c5-a36a-b52cee0a37a8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 4344 images belonging to 4 classes.\n",
            "Found 946 images belonging to 4 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m272/272\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m862s\u001b[0m 3s/step - accuracy: 0.2617 - loss: 1.5373 - val_accuracy: 0.0465 - val_loss: 1.3691\n",
            "Epoch 2/20\n",
            "\u001b[1m272/272\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 738ms/step - accuracy: 0.1480 - loss: 1.3155 - val_accuracy: 0.1332 - val_loss: 1.3695\n",
            "Epoch 3/20\n",
            "\u001b[1m272/272\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 738ms/step - accuracy: 0.1920 - loss: 1.2657 - val_accuracy: 0.0930 - val_loss: 1.3415\n",
            "Epoch 4/20\n",
            "\u001b[1m272/272\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 739ms/step - accuracy: 0.2834 - loss: 1.2143 - val_accuracy: 0.1015 - val_loss: 1.3371\n",
            "Epoch 5/20\n",
            "\u001b[1m272/272\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 741ms/step - accuracy: 0.3033 - loss: 1.2198 - val_accuracy: 0.3922 - val_loss: 1.3779\n",
            "Epoch 6/20\n",
            "\u001b[1m272/272\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 739ms/step - accuracy: 0.4877 - loss: 1.1875 - val_accuracy: 0.4926 - val_loss: 1.1752\n",
            "Epoch 7/20\n",
            "\u001b[1m272/272\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 739ms/step - accuracy: 0.5069 - loss: 1.0799 - val_accuracy: 0.4345 - val_loss: 1.2260\n",
            "Epoch 8/20\n",
            "\u001b[1m272/272\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 738ms/step - accuracy: 0.5072 - loss: 1.0635 - val_accuracy: 0.4387 - val_loss: 1.1912\n",
            "Epoch 9/20\n",
            "\u001b[1m272/272\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 734ms/step - accuracy: 0.5191 - loss: 0.9886 - val_accuracy: 0.3753 - val_loss: 1.3110\n",
            "Epoch 10/20\n",
            "\u001b[1m272/272\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 739ms/step - accuracy: 0.5097 - loss: 1.0089 - val_accuracy: 0.4249 - val_loss: 1.1688\n",
            "Epoch 11/20\n",
            "\u001b[1m272/272\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 740ms/step - accuracy: 0.5127 - loss: 1.0050 - val_accuracy: 0.4915 - val_loss: 1.0105\n",
            "Epoch 12/20\n",
            "\u001b[1m272/272\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 737ms/step - accuracy: 0.5029 - loss: 1.0152 - val_accuracy: 0.4958 - val_loss: 1.0185\n",
            "Epoch 13/20\n",
            "\u001b[1m272/272\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 739ms/step - accuracy: 0.5162 - loss: 0.9802 - val_accuracy: 0.5254 - val_loss: 0.9387\n",
            "Epoch 14/20\n",
            "\u001b[1m272/272\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 738ms/step - accuracy: 0.5308 - loss: 0.9143 - val_accuracy: 0.3277 - val_loss: 1.3667\n",
            "Epoch 15/20\n",
            "\u001b[1m272/272\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 746ms/step - accuracy: 0.5019 - loss: 0.9650 - val_accuracy: 0.4736 - val_loss: 1.0355\n",
            "Epoch 16/20\n",
            "\u001b[1m272/272\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m212s\u001b[0m 778ms/step - accuracy: 0.5300 - loss: 0.9326 - val_accuracy: 0.4609 - val_loss: 1.0574\n",
            "Epoch 17/20\n",
            "\u001b[1m272/272\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 773ms/step - accuracy: 0.5245 - loss: 0.9215 - val_accuracy: 0.4915 - val_loss: 1.0058\n",
            "Epoch 18/20\n",
            "\u001b[1m272/272\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 772ms/step - accuracy: 0.5161 - loss: 0.9506 - val_accuracy: 0.4947 - val_loss: 0.9815\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Green CNN trained and saved to /content/drive/MyDrive/Tomato_dataset/models/green_cnn(v2).h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "green_model.save('/content/drive/MyDrive/Tomato_dataset/models/green_cnn.keras')\n"
      ],
      "metadata": {
        "id": "x10a6l5xj4au"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "def evaluate_model(model_path, data_dir, img_size=(224, 224), batch_size=32):\n",
        "    print(f\"\\nğŸ”¹ Loading model from: {model_path}\")\n",
        "    model = tf.keras.models.load_model(model_path, compile=False)\n",
        "\n",
        "    datagen = ImageDataGenerator(rescale=1./255)\n",
        "    val_gen = datagen.flow_from_directory(\n",
        "        data_dir,\n",
        "        target_size=img_size,\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical',\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    print(f\"ğŸ”¹ Evaluating {len(val_gen.filenames)} images...\")\n",
        "    preds = model.predict(val_gen)\n",
        "    y_pred = np.argmax(preds, axis=1)\n",
        "    y_true = val_gen.classes\n",
        "\n",
        "    # Automatically detect available labels\n",
        "    class_labels = list(val_gen.class_indices.keys())\n",
        "    unique_true = np.unique(y_true)\n",
        "    unique_pred = np.unique(y_pred)\n",
        "    num_true_classes = len(unique_true)\n",
        "    num_pred_classes = preds.shape[1]\n",
        "\n",
        "    print(f\"\\nğŸ§© Detected {num_true_classes} true classes, {num_pred_classes} predicted classes.\")\n",
        "\n",
        "    # Adjust class_labels length to match the actual prediction shape\n",
        "    if len(class_labels) != num_pred_classes:\n",
        "        print(f\"âš ï¸ Adjusting label count from {len(class_labels)} â†’ {num_pred_classes}\")\n",
        "        class_labels = class_labels[:num_pred_classes]\n",
        "\n",
        "    # Metrics\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "    print(f\"\\nâœ… Accuracy: {acc:.4f}\")\n",
        "    print(f\"âœ… F1-score: {f1:.4f}\\n\")\n",
        "\n",
        "    print(\"ğŸ“Š Classification Report:\")\n",
        "    try:\n",
        "        print(classification_report(y_true, y_pred, target_names=class_labels, zero_division=0))\n",
        "    except ValueError:\n",
        "        print(\"âš ï¸ Warning: Adjusting label mismatch automatically.\")\n",
        "        print(classification_report(y_true, y_pred, labels=unique_true, zero_division=0))\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "                xticklabels=class_labels, yticklabels=class_labels)\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.ylabel(\"True Label\")\n",
        "    plt.xlabel(\"Predicted Label\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ğŸ”¸ Evaluate your models\n",
        "evaluate_model(\n",
        "    model_path=\"/content/drive/MyDrive/Tomato_dataset/models/green_cnn(v2).h5\",\n",
        "    data_dir=\"/content/drive/MyDrive/Tomato_dataset/cnn_crops/green/val\"\n",
        ")\n",
        "\n",
        "evaluate_model(\n",
        "    model_path=\"/content/drive/MyDrive/Tomato_dataset/models/ripe_cnn(v2).keras\",\n",
        "    data_dir=\"/content/drive/MyDrive/Tomato_dataset/cnn_crops/ripe/val\"\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        },
        "id": "L1uxY-Z0G7bO",
        "outputId": "0fd59380-70b9-4335-f557-8743e2b99a70"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ”¹ Loading model from: /content/drive/MyDrive/Tomato_dataset/models/green_cnn(v2).h5\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] Unable to synchronously open file (unable to open file: name = '/content/drive/MyDrive/Tomato_dataset/models/green_cnn(v2).h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-328960636.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;31m# ğŸ”¸ Evaluate your models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m evaluate_model(\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/Tomato_dataset/models/green_cnn(v2).h5\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mdata_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/Tomato_dataset/cnn_crops/green/val\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-328960636.py\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(model_path, data_dir, img_size, batch_size)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nğŸ”¹ Loading model from: {model_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mdatagen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageDataGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrescale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    194\u001b[0m         )\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".h5\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\".hdf5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         return legacy_h5_format.load_model_from_hdf5(\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/legacy/saving/legacy_h5_format.py\u001b[0m in \u001b[0;36mload_model_from_hdf5\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0mopened_new_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mopened_new_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, track_times, **kwds)\u001b[0m\n\u001b[1;32m    564\u001b[0m                                  \u001b[0mfs_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_strategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs_persist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_persist\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m                                  fs_threshold=fs_threshold, fs_page_size=fs_page_size)\n\u001b[0;32m--> 566\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to synchronously open file (unable to open file: name = '/content/drive/MyDrive/Tomato_dataset/models/green_cnn(v2).h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def count_images_per_class(data_dir):\n",
        "    class_counts = {}\n",
        "    for class_name in os.listdir(data_dir):\n",
        "        class_path = os.path.join(data_dir, class_name)\n",
        "        if os.path.isdir(class_path):\n",
        "            class_counts[class_name] = len(os.listdir(class_path))\n",
        "    return class_counts\n",
        "\n",
        "# Check image counts for ripe model\n",
        "ripe_train_dir = \"/content/drive/MyDrive/Tomato_dataset/cnn_crops/ripe/train\"\n",
        "ripe_val_dir = \"/content/drive/MyDrive/Tomato_dataset/cnn_crops/ripe/val\"\n",
        "print(\"ğŸ”¹ Image counts for ripe model train set:\")\n",
        "print(count_images_per_class(ripe_train_dir))\n",
        "print(\"ğŸ”¹ Image counts for ripe model validation set:\")\n",
        "print(count_images_per_class(ripe_val_dir))\n",
        "\n",
        "# Check image counts for green model\n",
        "green_train_dir = \"/content/drive/MyDrive/Tomato_dataset/cnn_crops/green/train\"\n",
        "green_val_dir = \"/content/drive/MyDrive/Tomato_dataset/cnn_crops/green/val\"\n",
        "print(\"\\nğŸ”¹ Image counts for green model train set:\")\n",
        "print(count_images_per_class(green_train_dir))\n",
        "print(\"\\nğŸ”¹ Image counts for green model validation set:\")\n",
        "print(count_images_per_class(green_val_dir))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rU8lRE9zG2kd",
        "outputId": "b432469f-0410-4e97-decf-063d5a1f5163"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”¹ Image counts for ripe model train set:\n",
            "{'R_healthy': 407, 'R_lateblight': 29, 'R_spots': 189, 'R_pests': 22, 'R_ber': 62}\n",
            "ğŸ”¹ Image counts for ripe model validation set:\n",
            "{'R_healthy': 113, 'R_lateblight': 6, 'R_spots': 66, 'R_pests': 3, 'R_ber': 22}\n",
            "\n",
            "ğŸ”¹ Image counts for green model train set:\n",
            "{'G_healthy': 1565, 'G_lateblight': 131, 'G_spots': 1038, 'G_pests': 7, 'G_ber': 64}\n",
            "\n",
            "ğŸ”¹ Image counts for green model validation set:\n",
            "{'G_healthy': 368, 'G_lateblight': 26, 'G_spots': 302, 'G_pests': 0, 'G_ber': 29}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve, f1_score, accuracy_score, cohen_kappa_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "def extended_evaluation(y_true, y_pred, y_prob, class_labels):\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "    kappa = cohen_kappa_score(y_true, y_pred)\n",
        "\n",
        "    print(f\"âœ… Accuracy: {acc:.4f}\")\n",
        "    print(f\"âœ… F1-score: {f1:.4f}\")\n",
        "    print(f\"âœ… Cohenâ€™s Kappa: {kappa:.4f}\")\n",
        "\n",
        "    print(\"\\nğŸ“Š Classification Report:\")\n",
        "    print(classification_report(y_true, y_pred, target_names=class_labels, zero_division=0))\n",
        "\n",
        "    # Confusion Matrix\n",
        "    plt.figure(figsize=(8,6))\n",
        "    sns.heatmap(confusion_matrix(y_true, y_pred), annot=True, fmt=\"d\",\n",
        "                xticklabels=class_labels, yticklabels=class_labels, cmap=\"Blues\")\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.show()\n",
        "\n",
        "    # ROC Curves for each class\n",
        "    plt.figure(figsize=(8,6))\n",
        "    for i, label in enumerate(class_labels):\n",
        "        fpr, tpr, _ = roc_curve(y_true == i, y_prob[:, i])\n",
        "        plt.plot(fpr, tpr, label=f\"{label} (AUC={auc(fpr, tpr):.2f})\")\n",
        "    plt.plot([0,1],[0,1],'k--')\n",
        "    plt.title(\"ROC Curves\")\n",
        "    plt.xlabel(\"False Positive Rate\")\n",
        "    plt.ylabel(\"True Positive Rate\")\n",
        "    plt.legend()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "kxAo0xmvo1Ag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "path = \"/content/drive/MyDrive/Tomato_dataset/models/\"\n",
        "print(\"Files in model folder:\\n\", os.listdir(path))\n"
      ],
      "metadata": {
        "id": "avUoZEnBqK68",
        "outputId": "94e3cffc-59b0-496d-c7f7-286396977dea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files in model folder:\n",
            " ['yolov8m_trained.pt', 'model.V2', 'ripe_cnn.h5', 'ripe_cnn.keras', 'green_cnn.h5', 'green_mobilenet.keras', 'ripe_mobilenet.keras', 'ripe_cnn(v2).h5', 'ripe_cnn(v2).keras']\n"
          ]
        }
      ]
    }
  ]
}